{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict students' dropout and academic success\n",
    "\n",
    "by Katherine Chen, Hancheng Qin, Yili Tang, Bill Wan\n",
    "2023/12/02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from myst_nb import glue\n",
    "import pickle\n",
    "#from sklearn import set_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In our study, we developed machine learning models, including SVM, Random Forest, and Logistic Regression (with L1 and L2 regularization), to predict the likelihood of student academic dropout in higher education. Due to a high number of features and their inter-correlations, our models initially exhibited overfitting. To address this, we implemented feature selection techniques (PCA and feature importance analysis) along with model's parameter optimization. The refined models demonstrated improved performance, evidenced by a narrow gap between training and testing accuracy. Among the three, SVM marginally outperformed the others, achieving an accuracy of 80% and an AUC score of 0.89. Nonetheless, there is potential for further enhancement in model performance through additional feature engineering and more extensive parameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the realm of educational analytics, understanding the factors that influence student performance is pivotal for shaping effective pedagogical strategies. Our project delves into this domain, leveraging the rich and multifaceted Student Performance Data Set from the UCI Machine Learning Repository {cite}`misc_student_performance_320`. This dataset, derived from two Portuguese secondary schools, offers a comprehensive view of various personal, social, and academic factors impacting student achievement in Mathematics and Portuguese language courses.\n",
    "\n",
    "Machine learning methodologies have been extensively used in educational data mining to detect patterns in large collections of educational data {cite}`EducationalDataMining2015`. Our objective is to utilize machine learning techniques to analyze and predict student academic outcomes, focusing primarily on identifying key predictors of success and risk factors for academic dropout. Through this analysis, we aim to uncover insights that can guide interventions and support mechanisms to enhance student performance. The dataset's inclusivity of attributes ranging from demographic backgrounds and family information to study habits and lifestyle choices provides a unique opportunity to explore the multifaceted nature of academic success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "### Data\n",
    "The data set used in this project is of student performance in secondary education (high school) of two Portuguese schools {cite}`misc_student_performance_320`. The data attributes include student grades, demographic, social and school related features, and it was collected by using school reports and questionnaires. Two datasets are provided regarding the performance in two distinct subjects: Mathematics and Portuguese language. The data set was sourced from the UCI Machine Learning Repository and can be found [here] (https://archive.ics.uci.edu/dataset/320/student+performance). Each row in the data set represents a studentâ€™s profile and academic outcomes, including the final grade (G3) and several other variables (e.g., school, sex, age, study time, absences, etc.).\n",
    "\n",
    "### Analysis\n",
    "The method we use include Random Forest, Logistic Regression, and SVM. In the landscape of machine learning, three algorithms stand out for their efficacy and versatility: Logistic Regression, Random Forest, and Support Vector Machine (SVM). We have employed the method of feature importance values and Principal Component Analysis (PCA) to streamline the dimensionality of our feature space. Data was split with 80% being partitioned into the training set and 20% being partitioned into the test set. The hyperparameter $K$ was chosen using 10-fold cross validation with the test score as the classification metric.  All numerical variables were standardized and categorical features were preprocessed by one-hot encoding just prior to model fitting. The Python programming language {cite}`Python`.  code used to perform the analysis and create this report can be found here: https://github.com/UBC-MDS/Student_Success_Predict_Group15. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results & Discussion\n",
    "\n",
    "To examine the potential of each predictor in forecasting student performance, we plotted the distributions of each predictor from the training data set and coloured the distribution by target class (graduate: green, dropout: orange, enroller: blue). In doing this we see that class distributions for most of the predictors overlap somewhat, but do show quite a difference in their centres and spreads. In particular, we come up with below observation in numeric features ({numref}`Figure{number} <numeric_feature_densities_by_class>`):\n",
    "1. Previous Qualification (Grade) and Admission Grade: Both these variables have similar ranges (min 95 to max 190), indicating a possible correlation between previous academic performance and admission grades. The mean and median values are close, suggesting a relatively symmetric distribution for these variables.\n",
    "2. Age at Enrollment: The age range is quite broad (17 to 70 years), indicating a diverse set of students in terms of age. Transformation technique like Standardization is required\n",
    "3. Curricular Units Credited (1st and 2nd Semesters): The mean values for credited curricular units in both semesters are low (around 0.71 for the 1st semester and 0.54 for the 2nd), the 75% percentile is 0, suggesting that most students do not have many, if any, units credited. This could be because they are first year students.\n",
    "\n",
    "While for categorical features, we come up with the below conclusion ({numref}`Figure{number} <distribution_of_the_categorical_features>`):\n",
    "1. Nationality: The majority are Portuguese, with a small representation from other nationalities.\n",
    "2. Parents' Occupation: Both mother's and father's occupations are coded numerically. The most common occupation code for mothers and fathers is Unskilled Workers. Be careful about the matrix sparsity issue.\n",
    "3. Debtor, Tuition Fees Up to Date, Scholarship Holder: There's a notable number of students who are debtors (397) or whose tuition fees are not up to date (419), while 871 are scholarship holders. These figures highlight the financial aspects and challenges faced by the student population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../results/figures/density_of_numeric_feature.png\n",
    "---\n",
    "width: 800px\n",
    "name: numeric_feature_densities_by_class\n",
    "---\n",
    "```\n",
    "The densities of numeric features by class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../results/figures/distribution_of_categorical_feature.png\n",
    "---\n",
    "width: 800px\n",
    "name: distribution_of_the_categorical_features\n",
    "---\n",
    "```\n",
    "The distribution of categorical features by class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not done below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We chose to use a simple classification model using the k-nearest neighbours algorithm. To find the model that best predicted whether a tumour was benign or malignant, we performed 30-fold cross validation using F2 score (beta = 2) as our metric of model prediction performance to select K (number of nearest neighbours). We observed that the optimal K was {glue:text}`best_k` ({numref}`Figure {number} <cancer_choose_k>`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../results/figures/cancer_choose_k.png\n",
    "---\n",
    "width: 600px\n",
    "name: cancer_choose_k\n",
    "---\n",
    "Results from 30-fold cross validation to choose K. F2 score (with beta = 2) was used as the classification metric as K was varied.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{glue:figure} test_scores_df\n",
    ":figwidth: 400px\n",
    ":name: \"test_scores_df\"\n",
    "\n",
    "Accuracy and F2 (beta = 2) score from best model predicted on the test data.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our prediction model performed quite well on test data, with a final overall accuracy of {glue:text}`accuracy` and F2 (beta = 2) score of {glue:text}`f2`. Other indicators that our model performed well come from the confusion matrix, where it only made {glue:text}`false_neg` mistakes. However all {glue:text}`false_neg` mistakes were predicting a malignant tumour as benign, given the implications this has for patients health, this model is not good enough to yet implement in the clinic ({numref}`Figure {number} <confusion_df>`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{glue:figure} confusion_df\n",
    ":figwidth: 650px\n",
    ":name: \"confusion_df\"\n",
    "\n",
    "Confusion matrix of model performance on test data.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further improve this model in future with hopes of arriving one that could be used in the clinic, there are several things we can suggest. First, we could look closely at the {glue:text}`false_neg` misclassified observations and compare them to several observations that were classified correctly (from both classes). The goal of this would be to see which feature(s) may be driving the misclassification and explore whether any feature engineering could be used to help the model better predict on observations that it currently is making mistakes on. Additionally, we would try seeing whether we can get improved predictions using other classifiers. One classifier we might try is random forest forest because it automatically allows for feature interaction, where k-nn does not. Finally, we also might improve the usability of the model in the clinic if we output and report the probability estimates for predictions. If we cannot prevent misclassifications through the approaches suggested above, at least reporting a probability estimates for predictions would allow the clinician to know how confident the model was in its prediction. Thus the clinician may then have the ability to perform additional diagnostic assays if the probability estimates for prediction of a given tumour class is not very high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{bibliography}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}